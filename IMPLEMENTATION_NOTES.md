# Implementation Discussion Notes

These notes highlight key discussion points about the web crawler implementation.

## Core Implementation Choices

### 1. Breadth-First Search (BFS) vs Depth-First Search (DFS)

**Choice**: BFS using a queue

**Reasoning**:

- More predictable memory usage than recursive DFS
- Easier to reason about crawl order
- Better foundation for adding rate limiting
- Avoids potential stack overflow on deep sites

**Alternative considered**: Recursive DFS

- Pros: Simpler initial implementation
- Cons: Stack depth issues, harder to add rate limiting

### 2. Sequential vs Concurrent Crawling

**Choice**: Sequential (one request at a time)

**Reasoning**:

- Server-friendly (no request flooding)
- Simpler error handling
- Production systems should respect target servers
- Easier to debug and maintain

**If I were to add concurrency**:

```typescript
// Batch processing with limited concurrency
const BATCH_SIZE = 5;
while (queue.length > 0) {
  const batch = queue.splice(0, BATCH_SIZE);
  const results = await Promise.all(batch.map((url) => fetchAndProcess(url)));
  // Process results...
}
```

### 3. State Management

**Choice**: In-memory `Set` for visited URLs

**Data structures considered**:

- `Set<string>`: O(1) lookup, simple âœ… (chosen)
- `Map<string, boolean>`: Same performance, more verbose
- Array: O(n) lookup, poor for large sites
- Bloom filter: Probabilistic, memory efficient but complex

**For production scale**: Consider Redis or database-backed storage for distributed crawling.

### 4. URL Normalization Strategy

**Choice**: Remove trailing slashes and fragments, keep query params

**Example transformations**:

```
https://example.com/         â†’ https://example.com
https://example.com/page/    â†’ https://example.com/page
https://example.com/page#top â†’ https://example.com/page
https://example.com?id=1     â†’ https://example.com?id=1 (kept!)
```

**Why keep query params?**

- They often represent distinct content (e.g., `?page=2`)
- Safer to over-crawl than miss content
- Trade-off: May visit duplicate content if site uses session IDs

### 5. Subdomain Handling

**Choice**: Exact hostname matching only

```typescript
isSameSubdomain('https://monzo.com', 'https://community.monzo.com');
// Returns: false âœ…

isSameSubdomain('https://monzo.com', 'https://monzo.com/about');
// Returns: true âœ…
```

**Alternative**: Could match on base domain only (e.g., allow all `*.monzo.com`)

- Rejected: Requirements specify "one subdomain" explicitly

## Error Handling Strategy

### Graceful Degradation

All errors are handled non-fatally:

1. **Network errors**: Log and skip, continue crawling
2. **HTTP errors** (404, 500): Log but mark as visited (avoid retry loops)
3. **Invalid URLs**: Skip silently (many sites have malformed links)
4. **Non-HTML content**: Skip (no point parsing PDFs, images)

**Philosophy**: Partial results are better than no results.

### Error Categories

```typescript
// Example error flow:
try {
  const response = await fetch(url);
  if (!response.ok) {
    // HTTP error: log but continue
    console.error(`HTTP ${response.status}: ${url}`);
    return null;
  }
  // Success path...
} catch (error) {
  // Network error: log but continue
  console.error(`Network error: ${error.message}`);
  return null;
}
```

## Testing Philosophy

### What's Tested

- âœ… URL normalization edge cases
- âœ… Subdomain detection logic
- âœ… HTML link extraction
- âœ… Protocol filtering (http/https only)

### What's NOT Tested (and why)

- âŒ Actual network requests
  - Reason: Flaky, slow, external dependencies
  - Alternative: Unit test all business logic
- âŒ Integration tests with real websites
  - Reason: Unreliable, sites change
  - Alternative: Mock HTML responses in unit tests

### Test Coverage Strategy

```
Core business logic: 100% unit tested
Network layer: Tested via manual verification
Integration: Not automated (external dependency)
```

## Performance Characteristics

### Time Complexity

- URL normalization: **O(1)**
- Subdomain check: **O(1)**
- Link extraction: **O(n)** where n = links on page
- Overall crawl: **O(p Ã— l)** where p = pages, l = avg links

### Space Complexity

- Visited set: **O(p)** where p = unique pages
- Queue: **O(l)** where l = pending links
- Results array: **O(p Ã— l)** storing all data

### Bottlenecks

1. **Network I/O**: Main bottleneck (sequential requests)
2. **HTML parsing**: Minor overhead with JSDOM
3. **Memory**: Results stored in memory (fine for small-medium sites)

## Scalability Considerations

### Current Limitations

| Limitation          | Impact                    | Mitigation Strategy        |
| ------------------- | ------------------------- | -------------------------- |
| Sequential crawling | Slow on large sites       | Add batched concurrency    |
| In-memory storage   | RAM limits on huge sites  | Use Redis/database         |
| No rate limiting    | Could overwhelm servers   | Add delay between requests |
| No retry logic      | Transient failures missed | Add exponential backoff    |

### Scaling Strategy (if needed)

**For moderate scale (1,000-10,000 pages)**:

```typescript
// Add simple concurrency
const CONCURRENT_REQUESTS = 5;
const RATE_LIMIT_MS = 100;
```

**For large scale (10,000+ pages)**:

- Distributed crawling (multiple workers)
- Message queue (RabbitMQ, Kafka)
- Centralized state (Redis)
- Respect robots.txt
- Persistent storage (PostgreSQL)

## Dependencies Justification

### JSDOM

**Purpose**: HTML parsing  
**Why chosen**: Standard, well-maintained, spec-compliant  
**Alternatives**: Cheerio (faster but less accurate), parse5 (lower-level)  
**Trade-off**: JSDOM is heavier but more reliable for edge cases

### Vitest

**Purpose**: Testing framework  
**Why chosen**: Modern, fast, great TypeScript support  
**Alternatives**: Jest (older, slower), AVA (less popular)  
**Trade-off**: Vitest is newer but has great dev experience

### TypeScript

**Purpose**: Type safety  
**Why chosen**: Catches bugs at compile time, better IDE support  
**Trade-off**: Adds build step, but worth it for maintainability

## Production Readiness Checklist

### âœ… Implemented

- [x] Error handling for all failure modes
- [x] URL normalization to prevent duplicates
- [x] Protocol filtering (http/https only)
- [x] Content-type validation (HTML only)
- [x] Subdomain restriction
- [x] Comprehensive tests
- [x] Clear documentation
- [x] Clean code structure

### ðŸš§ Missing (but easily added)

- [ ] Rate limiting
- [ ] Request timeouts
- [ ] Retry logic with backoff
- [ ] Robots.txt respect
- [ ] User-agent configuration
- [ ] Depth limiting
- [ ] Logging levels
- [ ] Metrics/monitoring hooks

### ðŸ“Š For Large-Scale Production

- [ ] Distributed architecture
- [ ] Persistent storage
- [ ] Message queue
- [ ] Health checks
- [ ] Graceful shutdown
- [ ] Circuit breakers

## Discussion Questions to Anticipate

### 1. "Why not use a framework like Scrapy?"

> The requirements specifically stated not to use frameworks that handle crawling behind the scenes. Also, implementing from scratch demonstrates understanding of core concepts like graph traversal, state management, and HTTP.

### 2. "How would you handle very large websites?"

> Current implementation is fine for small-to-medium sites. For large sites:
>
> 1. Add concurrency with rate limiting
> 2. Use persistent storage (Redis/Postgres)
> 3. Implement depth limiting
> 4. Add distributed workers if needed

### 3. "What about robots.txt?"

> Not implemented currently (out of scope for 4-hour exercise). Production implementation would:
>
> 1. Fetch `/robots.txt` on first visit
> 2. Parse using robots.txt parser library
> 3. Check allowed paths before each request
> 4. Respect crawl-delay directive

### 4. "How do you prevent infinite loops?"

> Multiple safeguards:
>
> 1. Visited `Set` prevents re-crawling
> 2. URL normalization reduces duplicates
> 3. Subdomain checking prevents external loops
>    Could add: Max depth limit, max pages limit

### 5. "What's your error handling strategy?"

> Graceful degradation: errors logged but don't stop crawl. Philosophy is partial results better than no results. For production, would add:
>
> - Retry logic with exponential backoff
> - Circuit breakers for consistently failing domains
> - Dead letter queue for manual review

### 6. "How would you test this?"

> Current: Comprehensive unit tests for business logic
> Production additions:
>
> - Integration tests with mock HTTP server
> - Contract tests for HTML parsing
> - Load tests for performance baselines
> - Chaos testing for error handling

### 7. "How would you monitor this in production?"

> Add instrumentation:
>
> - Pages crawled per second
> - Error rates by type
> - Average page load time
> - Queue depth
> - Memory usage
> - Failed URLs for investigation

## Code Highlights for Discussion

### Clean Separation of Concerns

```
crawler.ts: Business logic (crawling, parsing, validation)
index.ts:   CLI interface and output formatting
```

### Testable Functions

Each function has single responsibility and is easily testable:

- `normalizeURL`: Pure function, no side effects
- `isSameSubdomain`: Pure function, no side effects
- `getLinksFromHTML`: Pure function (JSDOM is deterministic)
- `crawl`: Side effects isolated (network calls)

### Type Safety

TypeScript ensures compile-time safety:

```typescript
export interface CrawlResult {
  url: string;
  links: string[];
}
```

### Error Boundaries

Errors handled at appropriate levels:

- Network errors: In `fetchPage`
- Parse errors: In `getLinksFromHTML`
- Invalid URLs: Caught and skipped

## Alternative Implementations Considered

### 1. Recursive DFS (rejected)

```typescript
// Simpler but has stack depth issues
async function crawlRecursive(url: string, visited: Set<string>) {
  if (visited.has(url)) return;
  visited.add(url);
  const links = await fetchAndExtractLinks(url);
  for (const link of links) {
    await crawlRecursive(link, visited); // Could stack overflow!
  }
}
```

### 2. Event-driven architecture (over-engineered)

```typescript
// Too complex for this scope
class Crawler extends EventEmitter {
  // Would be great for large scale but overkill here
}
```

### 3. Worker threads (over-engineered)

```typescript
// Concurrent crawling with workers
// Great for performance but adds complexity
const worker = new Worker('./crawler-worker.js');
```

## Key Takeaways

1. **Simplicity over cleverness**: Code should be easy to understand and maintain
2. **Reliability over speed**: Server-friendly, robust error handling
3. **Testability**: Core logic is pure functions where possible
4. **Production-ready**: Real-world error handling, not just happy path
5. **Scalable foundation**: Architecture supports adding features without rewrite

The implementation demonstrates:

- Understanding of graph traversal (BFS)
- State management (visited set)
- Error handling (graceful degradation)
- Testing (comprehensive unit tests)
- Code organization (separation of concerns)
- Trade-off analysis (documented decisions)
